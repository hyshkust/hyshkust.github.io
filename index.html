<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yisheng He</title>
  
  <meta name="author" content="Yisheng He">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!--
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  -->
	<link rel="icon" href="images/yishenghe.png">
</head>


<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:77%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yisheng He (‰ΩïÁõäÂçá)</name>
              </p>
              <p>
                Yisheng He is a researcher at Alibaba. He obtained his Ph.D. at <a href="https://hkust.edu.hk">HKUST</a>,
                advised by Prof. <a class="cqf_href">Qifeng Chen</a>, Prof. <a class="ql_href">Long Quan</a>, and Dr. <a class="sj_href">Jian Sun</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:ethanheysh@gmail.com">Email</a> &nbsp/&nbsp
                <!--  <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp  -->
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=UM4qFCsAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                <a href="https://github.com/ethnhe">GitHub</a> <!-- &nbsp/&nbsp -->
                <!-- <a href="images/wechat.jpg">WeChat</a> -->
              </p>
              <p>
                We are now actively hiring research interns. If you are interested in working with me, feel free to email me your CV.
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/My_photo.jpg"><img style="width:100%;max-width:90%" alt="profile photo" src="images/My_photo.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in 3D Computer Vision, AIGC, Embodied AI, and Digital Avatar.
              </p>
              <br>
                * denotes equal contribution; ‚Ä† denotes corresponding author.
            </br>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Forge4D.png" alt="" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2509.24209">
                <papertitle>Forge4D: Feed-Forward 4D Human Reconstruction and Interpolation from Uncalibrated Sparse-View Videos</papertitle>
              </a>
              <br>
                Yingdong Hu*, <strong>Yisheng He</strong>*‚Ä†, Jinnan Chen, Weihao Yuan, Kejie Qiu, Zehong Lin, Siyu Zhu, Zilong Dong, Jun Zhang
              <br>
              <em>Preprint, 2025 </em>
              <br>
	      	      <a href="https://zhenliuzju.github.io/huyingdong/Forge4D/">project page</a> /
	      	      <a href="https://arxiv.org/abs/2509.24209">paper</a> /
		            <a href="https://github.com/zhenliuZJU/Forge4D">code</a> 
                <img alt="GitHub stars" style="vertical-align:text-top" src="https://img.shields.io/github/stars/zhenliuZJU/Forge4D?style=social">
              <p>
                Forge4D is the first feed-forward model for 4D human Gaussian reconstruction in real world metric scale, and enables novel-view and novel-time synthesis from uncalibrated sparse-view videos in an efficient streaming manner.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/PanoLAM.png" alt="" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2509.07552">
                <papertitle>PanoLAM: Large Avatar Model for Gaussian Full-Head Synthesis from One-shot Unposed Image</papertitle>
              </a>
              <br>
              	Peng Li*, <strong>Yisheng He</strong>*‚Ä†, Yingdong Hu, Yuan Dong, Weihao Yuan, Yuan Liu, Siyu Zhu, Gang Cheng, Zilong Dong, Yike Guo
              <br>
              <em>Preprint, 2025 </em>
              <br>
	      	      <a href="https://panolam.github.io/">project page</a> /
	      	      <a href="https://arxiv.org/abs/2509.07552">paper</a> /
		            <a href="https://github.com/aigc3d/LAM">code</a> 
                <!--<img alt="GitHub stars" style="vertical-align:text-top" src="https://img.shields.io/github/stars/aigc3d/LAM?style=social"> --> 
              <p>
                PanoLAM is a large avatar model for Gaussian full-head reconstruction from a single unposed image. It utilize coarse-to-fine and dual-branch frameworks that creates Gaussian full-head within a second.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/CoProSketch.png" alt="" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2504.08259">
                <papertitle>CoProSketch: Controllable and Progressive Sketch Generation with Diffusion Model</papertitle>
              </a>
              <br>
              	Ruohao Zhan*, Yijin Li*, <strong>Yisheng He</strong>, Shuo Chen, Yichen Shen, Xinyu Chen, Zilong Dong, Zhaoyang Huang, Guofeng Zhang
              <br>
              <em>ACMM, 2025 </em>
              <br>
	      	      <a href="https://arxiv.org/abs/2504.08259">paper</a> 
              <p>
                CoProSketch provides prominent controllability and details for sketch generation with diffusion models.
              </p>
            </td>
          </tr>

	        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/LAM.png" alt="" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2502.17796">
                <papertitle>LAM: Large Avatar Model for One-shot Animatable Gaussian Head</papertitle>
              </a>
              <br>
              	<strong>Yisheng He</strong>*, Xiaodong Gu*, Xiaodan Ye, Chao Xu, Zhengyi Zhao, Yuan Dong, Weihao Yuan, Zilong Dong, Liefeng Bo
              <br>
              <em>SIGGRAPH, 2025 </em>
              <br>
	      	      <a href="https://aigc3d.github.io/projects/LAM/">project page</a> /
	      	      <a href="https://arxiv.org/pdf/2502.17796">paper</a> /
		            <a href="https://github.com/aigc3d/LAM">code</a> 
                <img alt="GitHub stars" style="vertical-align:text-top" src="https://img.shields.io/github/stars/aigc3d/LAM?style=social"> 
              <p>
                LAM creates animatable Gaussian heads with one-shot images in a single forward pass, which can be reenacted and rendered on various platforms (including mobile phones) in real time.
              </p>
            </td>
          </tr>
		
	        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/LaMP.png" alt="" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2410.07093">
                <papertitle>LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning</papertitle>
              </a>
              <br>
              	Zhe Li, Weihao Yuan, <strong>Yisheng He</strong>, Lingteng Qiu, Shenhao Zhu, Xiaodong Gu, Weichao Shen, Yuan Dong, Zilong Dong, Laurence T. Yang
              <br>
              <em>ICLR, 2025 </em>
              <br>
	      	    <a href="https://aigc3d.github.io/LaMP/">project page</a> /
	      	    <a href="https://arxiv.org/pdf/2410.07093">paper</a> /
		          <a href="https://github.com/gentlefress/LaMP">code</a> 
              <img alt="GitHub stars" style="vertical-align:text-top" src="https://img.shields.io/github/stars/gentlefress/LaMP?style=social"> 
              <p>
                LaMP is a language-motion pretraining model that advances text-to-motion generation, motion-text retrieval, and motion captioning through aligned language-motion representation learning.
              </p>
            </td>
          </tr>

	        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/MulsMo.png" alt="" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2412.09901">
                <papertitle>MulSMo: Multimodal Stylized Motion Generation by Bidirectional Control Flow</papertitle>
              </a>
              <br>
              	Zhe Li, <strong>Yisheng He</strong>, Zhong Lei, Weichao Shen, Qi Zuo, Lingteng Qiu, Shenhao Zhu, Zilong Dong, Laurence T. Yang, Weihao Yuan
              <br>
              <em>Arxiv, 2025 </em>
              <br>
	      	    <a href="https://arxiv.org/pdf/2412.09901">paper</a> 
              <p>
                We build a bidirectional control flow between the style and the content for stylized motion generation and enable multimodal style control including text, image, and style motions.
              </p>
            </td>
          </tr>
		
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/GIC.png" alt="" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openreview.net/pdf/35d3fb34ac9b1b65eb96b7a01480e9b13895a855.pdf">
                <papertitle>Gaussian-Informed Continuum for Physical Property Identification and Simulation</papertitle>
              </a>
              <br>
              	Junhao Cai, Yuji Yang, Weihao Yuan, <strong>Yisheng He</strong>, Zilong Dong, Liefeng Bo, Hui Cheng, Qifeng Chen
              <br>
              <em>NeurIPS, 2024 <font color="red"><strong>(Oral Presentation)</strong></font></em>
              <br>
	      	    <a href="https://jukgei.github.io/project/gic/">project page</a> /
	      	    <a href="https://arxiv.org/pdf/2406.14927">paper</a> /
		          <a href="https://github.com/Jukgei/gic">code</a> 
              <img alt="GitHub stars" style="vertical-align:text-top" src="https://img.shields.io/github/stars/Jukgei/gic?style=social"> 
              <p>
                  We introduce a hybrid framework that leverages 3D Gaussian representation to advance physical property identification.
              </p>
            </td>
          </tr>
		
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/MoGenTS.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openreview.net/pdf?id=FisyQfoJCm">
                <papertitle>MoGenTS: Motion Generation based on Spatial-Temporal Joint Modeling</papertitle>
              </a>
              <br>
              Weihao Yuan*, <strong>Yisheng He</strong>*, Weichao Shen, Yuan Dong, Xiaodong Gu, Zilong Dong, Liefeng Bo, Qixing Huang
              <br>
              <em>NeurIPS, 2024</em>
              <br>
              <a href="https://openreview.net/pdf?id=FisyQfoJCm">paper</a>
              <p>
                We introduce a 2D joint VQVAE to quantize each joint instead of all joints into tokens. A spatial-temporal modeling framework with temporal-spatial 2D masking and 2D attention is also proposed for motion generation.
              </p>
            </td>
          </tr>
	  
	        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/freditor.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2404.02514">
                <papertitle>Freditor: High-Fidelity and Transferable NeRF Editing by Frequency Decomposition</papertitle>
              </a>
              <br>
              <strong>Yisheng He</strong>, Weihao Yuan, Siyu Zhu, Zilong Dong, Liefeng Bo, Qixing Huang
              <br>
              <em>ECCV, 2024</em>
              <br>
	            <a href="https://aigc3d.github.io/freditor/">project page</a> /
	            <a href="https://arxiv.org/pdf/2404.02514">paper</a> 
              <p>
                We enable high-fidelity, transferable, and intensity control for neural field editing. 
                <!-- The syn-to-real generalization capability and robustness towards calibration errors are also explored. -->
              </p>
            </td>
          </tr>
		
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sketch2nerf.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2401.14257v2">
                <papertitle>Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation</papertitle>
              </a>
              <br>
              Minglin Chen, Longguang Wang, Weihao Yuan, Yukun Wang, Zhe Sheng, <strong>Yisheng He</strong>, Zilong Dong, Liefeng Bo, Yulan Guo
              <br>
              <em>Arxiv, 2024</em>
              <br>
	            <a href="https://arxiv.org/pdf/2401.14257v2">paper</a> 
              <p>
                Our method synthesizes consistent 3D content with fine-grained sketch control. 
                <!-- The syn-to-real generalization capability and robustness towards calibration errors are also explored. -->
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/OV9D.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2403.12396">
                <papertitle>OV9D: Open-Vocabulary Category-Level 9D Object Pose and Size Estimation</papertitle>
              </a>
              <br>
              Junhao Cai*, <strong>Yisheng He</strong>*, Weihao Yuan, Siyu Zhu, Zilong Dong, Liefeng Bo, Qifeng Chen,
              <br>
              <em>IEEE Robotics and Automation Letters (RA-L), 2024</em>
              <br>
		          <a href="https://ov9d.github.io/">project page</a> /
		          <a href="https://arxiv.org/pdf/2403.12396">paper</a> /
		          <a href="https://github.com/caijunhao/ov9d">code</a> 
              <img alt="GitHub stars" style="vertical-align:text-top" src="https://img.shields.io/github/stars/caijunhao/ov9d?style=social"> 
              <p>
                We introduce a new problem: open-vocabulary 9D object pose and size estimation, a new dataset: OO3D-9D, and a new framework based on vision foundation model to tackle this problem.
                <!-- The syn-to-real generalization capability and robustness towards calibration errors are also explored. -->
              </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/selfcatposesize.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2203.02884.pdf">
                <papertitle>Towards Self-Supervised Category-Level Object Pose and Size Estimation</papertitle>
              </a>
              <br>
              <strong>Yisheng He</strong>, <a class="fhq_href">Haoqiang Fan</a>, <a class="hhb_href">Haibin Huang</a>, <a class="cqf_href">Qifeng Chen</a>, <a class="sj_href">Jian Sun</a>
              <br>
              <em>Arxiv, 2022</em>
              <br>
		          <a href="https://arxiv.org/abs/2203.02884">project page</a> / 
		          <a href="https://arxiv.org/abs/2203.02884">paper</a> 
              <p></p>
              <p>
                A self-supervised framework for category-level object pose and
                size estimation via differentiable shape deformation, registration, and rendering.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/fs6d.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="">
                <papertitle>FS6D: Few-Shot 6D Pose Estimation of Novel Objects</papertitle>
              </a>
              <br>
              <strong>Yisheng He</strong>, Yao Wang, <a class="fhq_href">Haoqiang Fan</a>, <a class="sj_href">Jian Sun</a>, <a class="cqf_href">Qifeng Chen</a>
              <br>
              <em>CVPR, 2022</em>
              <br>
							<a href="https://fs6d.github.io/">project page</a> / 
							<a href="https://arxiv.org/abs/2203.14628">paper</a> / 
							<a href="https://hkustconnect-my.sharepoint.com/:f:/g/personal/yhebk_connect_ust_hk/Ek9OaY-nmD1GqOZdqV05AbIBZqrPpGMqAZSqoqNHBps23Q?e=VF2Ozk">data</a> /
							<a href="https://github.com/ethnhe/FS6D">code</a> 
              <img alt="GitHub stars" style="vertical-align:text-top" src="https://img.shields.io/github/stars/ethnhe/FS6D-PyTorch?style=social">
              <p></p>
              <p>
                A new open-set few-shot 6D object pose estimation problem: 
                estimating the 6D pose of an unknown object by a few support views without CAD models and extra training. 
                A large-scale synthesis dataset for pre-training and benchmarks for future research.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ffb6d.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2103.02242.pdf">
                <papertitle>FFB6D: A Full Flow Bidirectional Fusion Network for 6D Pose Estimation</papertitle>
              </a>
              <br>
              <strong>Yisheng He</strong>, <a class="hhb_href">Haibin Huang</a>, <a class="fhq_href">Haoqiang Fan</a>, <a class="cqf_href">Qifeng Chen</a>, <a class="sj_href">Jian Sun</a>
              <br>
              <em>CVPR, 2021 <font color="red"><strong>(Oral Presentation)</strong></font> </em>
              <br>
							<a href="https://arxiv.org/abs/2103.02242">project page</a> / 
							<a href="https://arxiv.org/abs/2103.02242">paper</a> / 
							<a href="https://github.com/ethnhe/FFB6D">code</a> 
              <img alt="GitHub stars" style="vertical-align:text-top" src="https://img.shields.io/github/stars/ethnhe/FFB6D?style=social"> /
              <a href="https://www.youtube.com/watch?v=SSi2TnyD6Is">video (youtube)</a> /
              <a href="https://www.bilibili.com/video/BV1YU4y1a7Kp?from=search&seid=8306279574921937158">video (bilibili)</a>
              <p></p>
              <p>
                A generic full flow bidirectional fusion framework for RGBD representation learning,
                applied to joint instance semantic segmentation and 3D keypoint-based 6D pose estimation.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ishape.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2109.15068.pdf">
                <papertitle>iShape: A First Step Towards Irregular Shape Instance Segmentation</papertitle>
              </a>
              <br>
              <a href="https://github.com/DIYer22">Lei Yang</a>, Ziwei Yan, <strong>Yisheng He</strong>, Wei Sun, Zhenhang Huang, <a class="hhb_href">Haibin Huang</a>, <a class="fhq_href">Haoqiang Fan</a>
              <br>
              <em>arXiv, 2021</em>
              <br>
							<a href="https://ishape.github.io/">project page</a> / 
							<a href="https://arxiv.org/abs/2109.15068">paper</a> / 
							<a href="https://ishape.github.io/">code</a> / 
							<a href="https://ishape.github.io/#3-our-ishape-dataset">dataset</a> 
              <p></p>
              <p>
                A brand new dataset to promote the study of instance segmentation for objects with irregular shapes and 
                an affinity-based algorithm to tackle it.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/pvn3d.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1911.04231.pdf">
                <papertitle>PVN3D: A Deep Point-wise 3D Keypoints Voting Network for 6DoF Pose Estimation</papertitle>
              </a>
              <br>
              <strong>Yisheng He</strong>, Wei Sun, <a class="hhb_href">Haibin Huang</a>, Jianran Liu, <a class="fhq_href">Haoqiang Fan</a>, <a class="sj_href">Jian Sun</a>
              <br>
              <em>CVPR, 2020</em>
              <br>
							<a href="https://arxiv.org/abs/1911.04231">project page</a> /
							<a href="https://arxiv.org/abs/1911.04231">paper</a> / 
							<a href="https://github.com/ethnhe/PVN3D">code</a>
              <img alt="GitHub stars" style="vertical-align:text-top" src="https://img.shields.io/github/stars/ethnhe/PVN3D?style=social"> /
              <a href="https://www.youtube.com/watch?v=ZKo788cyD-Q">video (youtube)</a> /
              <a href="https://www.bilibili.com/video/av89408773/">video (bilibili)</a>
              <p></p>
              <p>The first deep learning 3D keypoint-based 6D pose estimation algorithm and an overall framework for joint instance semantic segmantation and 3D keypoint detection.</p>
            </td>
          </tr>
<!--
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Products</heading>
              <p>
                I've also worked on technology that transfers to industrial products while at Megvii Research and Microsoft.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/livenessdet.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.oppo.com/en/smartphones/series-find-x/find-x/">
                <papertitle>3D Face Recognition | Liveness Detection</papertitle>
              </a>
              <p></p>
              <p>
                We developed the world's first 3D face recognition algorithm for Android smartphones. 
                It's shipped with <a href='https://www.oppo.com/en/smartphone-find_x/'>OPPO Find X</a> announced on June 2018.
                I developed liveness detection algorithms based on depth and infrared (IR) images in the project.
              </p>
              <p>The project team won the annual Meg-Team award, 2018.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/microassist.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.microsoft.com/zh-cn/microsoft-365/business/weizhuli">
                <papertitle>Microsoft 365 Micro Asistant (Dragon-Gate)</papertitle>
              </a>
              <p></p>
              <p>We developed a set of office suites based on Microsoft Office 365 and WeChat. The product was announced on November, 2017.</p>
            </td>
          </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Experience</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Megvii_logo.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
                <papertitle>Megvii (Face++), Senior Research Intern, Jan.2019-</papertitle>
                <p>Supervisor: Dr. <a class="sj_href">Jian Sun</a>; Collaborators: Dr. <a class="hhb_href">Haibin Huang</a>, <a class="fhq_href">Haoqiang Fan</a></p>
                <p></p>
                <papertitle>Megvii (Face++), Research Intern, Dec.2017-May.2018</papertitle>
                <p>Mentors: Dr. <a href="https://bigeagle.me/about/">Yuzhi Wang</a>, <a class="fhq_href">Haoqiang Fan</a> </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/microsoft_logo.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Microsoft, SDE Intern, Jun.2017-Oct.2017</papertitle>
              <p>Mentors: <a href="https://www.linkedin.com/injobs/in/raymond-xue-8876b615">Raymond Xue</a>, Hao Lin</p>
            </td>
          </tr>
        </tbody></table>

-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading>Academic Challenge</heading>
                    </td>
                </tr>

            </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          </tbody>

                <tr>
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/OCRTOC.png" alt="clean-usnob" width="160" height="160">
                 </td>
                  <td width="75%" valign="middle">
                    <papertitle>
                      Rank 2nd in <a href="http://www.ocrtoc.org/">OCRTOC: Open Cloud Robot Table Organization Challenge</a> , 2020
                    </papertitle>
                  </td>
                </tr>
        </table>

        <!--
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading>Patents</heading>
                        <p></p>
                        <li> CN108921070A (Issued 2018.11), "Image processing, model training methods and corresponding devices".  </li>
                        <li> CN109191802A (Issued 2019.01), "Method, apparatus, system and storage medium for eye protection".  </li>
                        <li> CN112614134A (In process), "Image segmentation method, device, electronic device and storage medium". </li>
                    </td>
                </tr>
            </tbody>
        </table>
          
        -->


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading>Services</heading>
                        <p></p>
                        <li>Program Committee/Reviewers: CVPR, ICCV, ECCV, NeurIPS, ICLR, AAAI, ACMM, ICRA, IROS, TPAMI, IJCV, RAL, Neurocomputing </li>
                        <li>Teaching Assistant @ HKUST:  COMP 4201 (Spring 2019), COMP 1029 (Fall 2020), COMP 4201 (Spring 2021) </li>
                    </td>
                </tr>
            </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Last updated: October, 2025.
              </p>
              <p style="text-align:right;font-size:small;">
                Thanks Dr. <a href="https://jonbarron.info/">Jon Barron</a> for sharing the <a href="https://github.com/jonbarron/jonbarron_website">template code</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>

<script>
var sj_href = "https://scholar.google.com/citations?user=ALVSZAYAAAAJ&hl=en";
var ql_href = "https://www.cse.ust.hk/~quan/";
var cqf_href = "https://cqf.io";
var fhq_href = "https://scholar.google.com/citations?user=bzzBut4AAAAJ";
var hhb_href = "https://brotherhuang.github.io";

function set_cls_href(cls_nm, href){
var x = document.getElementsByClassName(cls_nm);
var i;
for (i = 0; i < x.length; i++) {
    x[i].setAttribute("href", href);
}
}

set_cls_href("sj_href", sj_href);
set_cls_href("ql_href", ql_href);
set_cls_href("cqf_href", cqf_href);
set_cls_href("fhq_href", fhq_href);
set_cls_href("hhb_href", hhb_href);
</script>
